{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kilzol/Amazon-web-services/blob/master/presentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PRE PROCESSING/ DATA CLEANING**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I8JiQFjc_ZBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tsurumeso/vocal-remover.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CRYOLorRs3U",
        "outputId": "94de1d06-d1fc-47bc-e388-4f0f138b647f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vocal-remover'...\n",
            "remote: Enumerating objects: 731, done.\u001b[K\n",
            "remote: Counting objects: 100% (159/159), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 731 (delta 129), reused 98 (delta 98), pack-reused 572 (from 2)\u001b[K\n",
            "Receiving objects: 100% (731/731), 186.56 KiB | 5.49 MiB/s, done.\n",
            "Resolving deltas: 100% (433/433), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd vocal-remover"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ3DCSwcSoCV",
        "outputId": "436250ac-b613-40dc-d4fc-445847233122"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/vocal-remover\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install resampy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tQGk7LRWe5-",
        "outputId": "46fb32bb-7862-403f-edbf-691cfedbbcc9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting resampy\n",
            "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from resampy) (2.0.2)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.11/dist-packages (from resampy) (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.53->resampy) (0.43.0)\n",
            "Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: resampy\n",
            "Successfully installed resampy-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0730 H  ON 02 AUG 2023 (08) (2)**"
      ],
      "metadata": {
        "id": "4Dp9vX5r1zIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --input /content/videoplayback.mp4 --output_dir output --gpu 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vIhTrltTBGv",
        "outputId": "acb246c2-2521-4882-e9b0-af56b6445bd6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading model... done\n",
            "/content/vocal-remover/inference.py:136: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  X, sr = librosa.load(\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "loading wave source... done\n",
            "stft of wave source... done\n",
            "100% 25/25 [00:04<00:00,  5.10it/s]\n",
            "validating output directory... done\n",
            "inverse stft of instruments... done\n",
            "inverse stft of vocals... done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0800 H   02 AUG 2023 (01)**"
      ],
      "metadata": {
        "id": "XjKDGALC7Rdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --input /content/audio2.wav --output_dir output --gpu 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7TlzhXB7ZzJ",
        "outputId": "6fdbfbff-fefe-449e-e055-2aacb27899e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading model... done\n",
            "loading wave source... done\n",
            "stft of wave source... done\n",
            "100% 7/7 [02:00<00:00, 17.17s/it]\n",
            "validating output directory... done\n",
            "inverse stft of instruments... done\n",
            "inverse stft of vocals... done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0805 H  02 AUG 2023 (02)**"
      ],
      "metadata": {
        "id": "Jn2RJZGKNSb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --input /content/audio3.wav --output_dir output --gpu 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhOjOgOyNdOt",
        "outputId": "5137d3cf-bc88-490b-8040-23dd93e4274d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading model... done\n",
            "loading wave source... done\n",
            "stft of wave source... done\n",
            "100% 7/7 [01:48<00:00, 15.46s/it]\n",
            "validating output directory... done\n",
            "inverse stft of instruments... done\n",
            "inverse stft of vocals... done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0810  ON 02 AUG 2023 (03) (1)**"
      ],
      "metadata": {
        "id": "90HrHgfxeawN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --input /content/audio4.wav --output_dir output --gpu 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EM0vJBZceedN",
        "outputId": "29c63af3-d33c-4775-d759-b9c5b42ae7c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading model... done\n",
            "loading wave source... done\n",
            "stft of wave source... done\n",
            "100% 4/4 [01:10<00:00, 17.54s/it]\n",
            "validating output directory... done\n",
            "inverse stft of instruments... done\n",
            "inverse stft of vocals... done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0915 H   02 AUG 2023 (01) (1)**\n"
      ],
      "metadata": {
        "id": "WLpbSlr-0BtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --input /content/audio5.wav --output_dir output --gpu 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D3PUiBD0KBR",
        "outputId": "69cbd958-84b1-499d-c9e4-a6c216e4d27e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading model... done\n",
            "loading wave source... done\n",
            "stft of wave source... done\n",
            "100% 6/6 [00:01<00:00,  3.24it/s]\n",
            "validating output directory... done\n",
            "inverse stft of instruments... done\n",
            "inverse stft of vocals... done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0950 H 02 AUG 2023 (01) (1)**"
      ],
      "metadata": {
        "id": "f3cuQsHUzjgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --input /content/audio6.wav --output_dir output --gpu 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud6DUpREzk8H",
        "outputId": "96ef5afe-1ea2-406d-f658-e83b7ac8515e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading model... done\n",
            "loading wave source... done\n",
            "stft of wave source... done\n",
            "100% 4/4 [00:01<00:00,  2.56it/s]\n",
            "validating output directory... done\n",
            "inverse stft of instruments... done\n",
            "inverse stft of vocals... done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1350 H 02 AUG 2023 (01)**"
      ],
      "metadata": {
        "id": "egsTa3pa0J2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --input /content/audio7.wav --output_dir output --gpu 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrSJct6m0OPV",
        "outputId": "ca6df5f2-72f3-4f04-dcab-76c39c561fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading model... done\n",
            "loading wave source... done\n",
            "stft of wave source... done\n",
            "100% 11/11 [00:03<00:00,  3.57it/s]\n",
            "validating output directory... done\n",
            "inverse stft of instruments... done\n",
            "inverse stft of vocals... done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1405 H   AS ON 02 AUG 2023 (01) (1)**"
      ],
      "metadata": {
        "id": "i4na7T5s0Tdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --input /content/audio8.wav --output_dir output --gpu 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ0A70oR0V4S",
        "outputId": "f47cd684-912f-4b3e-eaf0-ce4f2800e9e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading model... done\n",
            "loading wave source... done\n",
            "stft of wave source... done\n",
            "100% 8/8 [00:02<00:00,  3.68it/s]\n",
            "validating output directory... done\n",
            "inverse stft of instruments... done\n",
            "inverse stft of vocals... done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1635 H   AS ON 02 AUG 2023 (01) (1)**"
      ],
      "metadata": {
        "id": "hSDawipF1pBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --input /content/audio9.wav --output_dir output --gpu 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA230Tgu1s1J",
        "outputId": "a0c95cce-8855-4848-a680-b84deb765117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading model... done\n",
            "loading wave source... done\n",
            "stft of wave source... done\n",
            "100% 8/8 [00:02<00:00,  3.73it/s]\n",
            "validating output directory... done\n",
            "inverse stft of instruments... done\n",
            "inverse stft of vocals... done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1640 H  AS ON 02 AUG 2023 (02) (1)**"
      ],
      "metadata": {
        "id": "FWGPr7es15gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --input /content/audio10.wav --output_dir output --gpu 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5xvsvXN18ZU",
        "outputId": "d941cbd5-68e1-43f1-f42f-5600de37af7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading model... done\n",
            "loading wave source... done\n",
            "stft of wave source... done\n",
            "100% 5/5 [00:01<00:00,  3.01it/s]\n",
            "validating output directory... done\n",
            "inverse stft of instruments... done\n",
            "inverse stft of vocals... done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SPEECH TO TEXT USING WHISPER.AI**\n"
      ],
      "metadata": {
        "id": "u3I0fIQuRwCB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zilq2Lrp-f9c",
        "outputId": "1d659f74-93f3-4746-b67c-5c1d80764b80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-r35g1bsv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-r35g1bsv\n",
            "  Resolved https://github.com/openai/whisper.git to commit dd985ac4b90cafeef8712f2998d62c59c3e62d22\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803705 sha256=bdb3269682c15d4b89c1d339abcff8347ac8446ad69bd3d6a9144aa542059c4b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0y3xpjeq/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,683 kB]\n",
            "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,958 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,729 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [77.3 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,540 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,387 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,552 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,944 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,255 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\n",
            "Fetched 31.9 MB in 5s (6,208 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "89 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 89 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "id": "0akiG4rXznci",
        "outputId": "dad7949b-65ad-42a5-8034-2d57ead918d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-teaxfri1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-teaxfri1\n",
            "  Resolved https://github.com/openai/whisper.git to commit dd985ac4b90cafeef8712f2998d62c59c3e62d22\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0730 H  ON 02 AUG 2023 (08) (2)**"
      ],
      "metadata": {
        "id": "7ZOjRrZzCZ8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper /content/vocal-remover/output/videoplayback_Vocals.wav --task translate --model medium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1oEaGGz-8iZ",
        "outputId": "d056be99-bb8a-49cd-9492-37a83689caf2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:16<00:00, 94.1MiB/s]\n",
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: Chinese\n",
            "[00:00.000 --> 00:04.000]  Hello everyone, we are Easy Taiwanese Mandarin.\n",
            "[00:04.000 --> 00:06.000]  I am the host today, Zhu Zhu.\n",
            "[00:06.000 --> 00:13.000]  Today we came to Taipei Yanshan Flower Park to ask if Taiwanese people like Taiwan.\n",
            "[00:13.000 --> 00:17.000]  Now we are going to start our interview. Please come with me.\n",
            "[00:30.000 --> 00:45.000]  First question, are you Taiwanese?\n",
            "[00:45.000 --> 00:46.000]  Yes.\n",
            "[00:46.000 --> 00:47.000]  Do you like Taiwan?\n",
            "[00:47.000 --> 00:48.000]  I like it.\n",
            "[00:48.000 --> 00:49.000]  Why?\n",
            "[00:49.000 --> 00:52.000]  There are a lot of things to eat here.\n",
            "[00:52.000 --> 00:54.000]  I like Taiwan.\n",
            "[00:54.000 --> 00:58.000]  Where do you like Taiwan? Why?\n",
            "[00:58.000 --> 01:07.000]  I think Taiwan is a place full of food and people's enthusiasm.\n",
            "[01:07.000 --> 01:11.000]  I like Taiwan very much. I like Taiwan very much.\n",
            "[01:11.000 --> 01:12.000]  Why?\n",
            "[01:12.000 --> 01:13.000]  Why?\n",
            "[01:13.000 --> 01:20.000]  I am Taiwanese. I am a local Taiwanese. I will definitely like Taiwan. I love my own land.\n",
            "[01:20.000 --> 01:22.000]  Are you Taiwanese?\n",
            "[01:22.000 --> 01:25.000]  I am not Taiwanese. I came from Hong Kong.\n",
            "[01:25.000 --> 01:28.000]  I like Taiwan very much because this is my home.\n",
            "[01:28.000 --> 01:30.000]  I like Taiwan very much.\n",
            "[01:30.000 --> 01:32.000]  Where do you like Taiwan?\n",
            "[01:32.000 --> 01:34.000]  I think Taiwan has a lot of food.\n",
            "[01:34.000 --> 01:38.000]  I can't eat this kind of Taiwanese food in other countries.\n",
            "[01:38.000 --> 01:44.000]  There is a beautiful weather, a lot of mountains, a lot of sea, and all kinds of different terrains.\n",
            "[01:44.000 --> 01:47.000]  I think Taiwan is a very beautiful place.\n",
            "[01:47.000 --> 01:48.000]  Do you like Taiwan?\n",
            "[01:48.000 --> 01:51.000]  Not bad. I like it.\n",
            "[01:51.000 --> 01:53.000]  I like it.\n",
            "[01:53.000 --> 01:56.000]  Which part of Taiwan do you like?\n",
            "[01:56.000 --> 01:58.000]  Which part?\n",
            "[01:58.000 --> 02:01.000]  I like night markets.\n",
            "[02:01.000 --> 02:06.000]  I like places where Taiwan is not polluted.\n",
            "[02:06.000 --> 02:08.000]  Where is it like?\n",
            "[02:08.000 --> 02:12.000]  It's like a part of the East China Sea.\n",
            "[02:12.000 --> 02:16.000]  I like Taiwan's food and people.\n",
            "[02:16.000 --> 02:18.000]  I like it.\n",
            "[02:18.000 --> 02:21.000]  The air is nice.\n",
            "[02:21.000 --> 02:23.000]  The place is good.\n",
            "[02:23.000 --> 02:25.000]  It's easy to travel.\n",
            "[02:25.000 --> 02:27.000]  I like Taiwan.\n",
            "[02:27.000 --> 02:29.000]  It's very free and there are a lot of places to play.\n",
            "[02:29.000 --> 02:31.000]  It's very convenient to go up and down the sea.\n",
            "[02:31.000 --> 02:33.000]  People are very polite.\n",
            "[02:33.000 --> 02:37.000]  They are kind of enthusiastic to help each other.\n",
            "[02:37.000 --> 02:39.000]  Of course, the food is delicious.\n",
            "[02:39.000 --> 02:44.000]  And it's really free to do whatever you want.\n",
            "[02:45.000 --> 02:47.000]  Are you Taiwanese?\n",
            "[02:47.000 --> 02:49.000]  I like Taiwan.\n",
            "[02:49.000 --> 02:53.000]  I am not Taiwanese.\n",
            "[02:53.000 --> 02:55.000]  I am from Taiwan.\n",
            "[02:55.000 --> 02:57.000]  Is this your first time here?\n",
            "[02:57.000 --> 02:59.000]  This is my third time.\n",
            "[02:59.000 --> 03:01.000]  I've been here three times.\n",
            "[03:01.000 --> 03:03.000]  I've been here for two months.\n",
            "[03:03.000 --> 03:05.000]  I've been here with the tour group.\n",
            "[03:05.000 --> 03:07.000]  Of course, I like it.\n",
            "[03:07.000 --> 03:09.000]  I've been abroad a few times.\n",
            "[03:09.000 --> 03:11.000]  I think Taiwan is the best.\n",
            "[03:11.000 --> 03:13.000]  What's the difference from Hong Kong?\n",
            "[03:13.000 --> 03:19.000]  Hong Kong has a little bit of the same taste.\n",
            "[03:19.000 --> 03:21.000]  I like Taiwan.\n",
            "[03:21.000 --> 03:23.000]  Where do you like Taiwan?\n",
            "[03:23.000 --> 03:28.000]  I like Taipei and Hualien.\n",
            "[03:28.000 --> 03:29.000]  I like it.\n",
            "[03:29.000 --> 03:31.000]  Where do you like Taiwan the most?\n",
            "[03:31.000 --> 03:32.000]  Guangfu.\n",
            "[03:32.000 --> 03:34.000]  Why?\n",
            "[03:34.000 --> 03:39.000]  Because Guangfu has a lot of people and cars.\n",
            "[03:39.000 --> 03:42.000]  Where do you like Taiwan?\n",
            "[03:43.000 --> 03:48.000]  Do you like Hong Kong more than Taiwan or Hong Kong?\n",
            "[03:48.000 --> 03:51.000]  Because Hong Kong is a long time away.\n",
            "[03:51.000 --> 03:55.000]  Of course, I like Hong Kong the most.\n",
            "[03:55.000 --> 03:57.000]  My family is in Hong Kong.\n",
            "[03:57.000 --> 04:00.000]  Are you happy in Taiwan?\n",
            "[04:00.000 --> 04:06.000]  I am very happy in Taiwan.\n",
            "[04:06.000 --> 04:09.000]  Do you want to come to Taiwan again?\n",
            "[04:09.000 --> 04:11.000]  Yes, I will come.\n",
            "[04:11.000 --> 04:13.000]  I will come again.\n",
            "[04:13.000 --> 04:14.000]  Let's go home.\n",
            "[04:14.000 --> 04:16.000]  Why?\n",
            "[04:18.000 --> 04:20.000]  Why?\n",
            "[04:20.000 --> 04:22.000]  I like it.\n",
            "[04:22.000 --> 04:27.000]  Thank you for watching today's Easy Taiwanese Mandarin.\n",
            "[04:27.000 --> 04:30.000]  If you have any questions, please leave a message below.\n",
            "[04:30.000 --> 04:32.000]  And subscribe to our channel.\n",
            "[04:32.000 --> 04:33.000]  See you next time.\n",
            "[04:33.000 --> 04:35.000]  Thank you.\n",
            "[04:39.000 --> 04:42.000]  Thank you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0730 H  ON 02 AUG 2023 (08) (2)    SPECIFING THE LANGUAGE**"
      ],
      "metadata": {
        "id": "MtS4Q1Zt3Rug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper /content/videoplayback.mp4 --language Chinese --task translate"
      ],
      "metadata": {
        "id": "tWJCahCXA3c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28592066-86d6-4311-c739-272394d48807"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/whisper\", line 8, in <module>\n",
            "    sys.exit(cli())\n",
            "             ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py\", line 595, in cli\n",
            "    model = load_model(model_name, device=device, download_root=model_dir)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/whisper/__init__.py\", line 154, in load_model\n",
            "    model = Whisper(dims)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/whisper/model.py\", line 256, in __init__\n",
            "    self.encoder = AudioEncoder(\n",
            "                   ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/whisper/model.py\", line 184, in __init__\n",
            "    [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/whisper/model.py\", line 184, in <listcomp>\n",
            "    [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/whisper/model.py\", line 156, in __init__\n",
            "    Linear(n_state, n_mlp), nn.GELU(), Linear(n_mlp, n_state)\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 112, in __init__\n",
            "    self.reset_parameters()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 118, in reset_parameters\n",
            "    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/init.py\", line 518, in kaiming_uniform_\n",
            "    return tensor.uniform_(-bound, bound, generator=generator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0800 H 02 AUG 2023 (01)**\n"
      ],
      "metadata": {
        "id": "tXt1FMXL-cSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio2_Vocals.wav --task translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLw-XQRN-nlk",
        "outputId": "5e06bfa7-2ffc-43a9-d1df-669a045d9b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: Javanese\n",
            "[00:00.000 --> 00:30.000]  I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm\n",
            "[00:30.000 --> 00:45.760]  saying, I don't know what I'm saying, I don't know what I'm saying, I don't know what I'm saying,\n",
            "[00:45.760 --> 00:52.340]  No, I have been\n",
            "[00:52.340 --> 01:02.620]  there for a long time.\n",
            "[01:02.620 --> 01:07.760]  Yeah.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0800 H 02 AUG 2023 (01) SPECIFING LANGUAGE**"
      ],
      "metadata": {
        "id": "c4ml1rQC_ApX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio2_Vocals.wav --language Chinese --task translate"
      ],
      "metadata": {
        "id": "ZuLk_4IP_V2v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "174ec880-851c-4518-d261-9a86b7345d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "[00:00.000 --> 00:30.000]  I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do, I don't know what to do\n",
            "[00:30.000 --> 00:40.920]  in your life, what does it mean to say you will see your lyre ?\n",
            "[00:40.920 --> 00:48.380]  I can't understand it but I don't know what I believe, the truth is true, many run嫉.\n",
            "[00:49.820 --> 00:55.900]  but in my life, I did not believe so much, I didn't believe so much, it was funny.\n",
            "[00:55.900 --> 01:05.460]  You forget for a moment, that you're a正確 person!\n",
            "[01:05.460 --> 01:07.640]  Just normal understanding\n",
            "[01:07.660 --> 01:11.220] 真正確實!\n",
            "[01:11.220 --> 01:13.900]  When there is such a matter, mine becomes false!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0805 H 02 AUG 2023 (02)**"
      ],
      "metadata": {
        "id": "QnpLs1ZrQxm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio3_Vocals.wav --task translate"
      ],
      "metadata": {
        "id": "anohJSb_c-H9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c4ac6fc-13e0-4732-c45a-ba5d5520f32b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: Myanmar\n",
            "[00:00.000 --> 00:02.000]  we are\n",
            "[00:02.180 --> 00:05.440]  he broader real world\n",
            "[00:10.380 --> 00:13.620]  I denied him\n",
            "[00:24.720 --> 00:27.040]  not real material\n",
            "[00:32.000 --> 00:34.000]  I'm going to show you how to do it.\n",
            "[00:34.000 --> 00:36.000]  I'm going to show you how to do it.\n",
            "[00:36.000 --> 00:38.000]  I'm going to show you how to do it.\n",
            "[00:38.000 --> 00:40.000]  I'm going to show you how to do it.\n",
            "[00:40.000 --> 00:42.000]  I'm going to show you how to do it.\n",
            "[00:42.000 --> 00:44.000]  I'm going to show you how to do it.\n",
            "[00:44.000 --> 00:46.000]  I'm going to show you how to do it.\n",
            "[00:46.000 --> 00:48.000]  I'm going to show you how to do it.\n",
            "[00:48.000 --> 00:50.000]  I'm going to show you how to do it.\n",
            "[00:50.000 --> 00:52.000]  I'm going to show you how to do it.\n",
            "[00:52.000 --> 00:54.000]  I'm going to show you how to do it.\n",
            "[00:54.000 --> 00:56.000]  I'm going to show you how to do it.\n",
            "[00:56.000 --> 00:58.000]  I'm going to show you how to do it.\n",
            "[00:58.000 --> 01:00.000]  I'm going to show you how to do it.\n",
            "[01:00.000 --> 01:02.000]  I'm going to show you how to do it.\n",
            "[01:02.000 --> 01:04.000]  I'm going to show you how to do it.\n",
            "[01:07.000 --> 01:09.000]  I'm going to show you how to do it.\n",
            "[01:09.000 --> 01:11.000]  I'm going to show you how to do it.\n",
            "[01:11.000 --> 01:13.000]  I'm going to show you how to do it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0805 H 02 AUG 2023 (02) SPECIFING THE LANGUAGE**"
      ],
      "metadata": {
        "id": "jzqS8OPeSJo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio3_Vocals.wav --language Chinese --task translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95fNLtv1SWYi",
        "outputId": "160d462a-3558-43c6-da6a-d41f8f955726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "[00:00.000 --> 00:04.820]  you can see here, Chairman Li visited the system\n",
            "[00:05.060 --> 00:07.860]  and I watched the notes\n",
            "[00:07.960 --> 00:09.760]  I remembered all that\n",
            "[00:09.900 --> 00:19.580]  and when we were small, pretty little\n",
            "[00:20.040 --> 00:26.640]  She told me she was still doing it\n",
            "[00:26.640 --> 00:56.640]  I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm\n",
            "[00:56.640 --> 01:12.600]  doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing, I don't know what I'm doing,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0810 ON 02 AUG 2023 (03) (1)**"
      ],
      "metadata": {
        "id": "WQ1T5pDOi2xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio4_Vocals.wav --task translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehkGm0w6i_Ji",
        "outputId": "59a51299-796d-48b8-c73b-e9d6769c3a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: Myanmar\n",
            "[00:00.000 --> 00:30.000]  I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't\n",
            "[00:30.000 --> 00:46.460]  know what you're talking about, I don't know what you're talking about, I don't know what you're talking okay okay okay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0810 ON 02 AUG 2023 (03) (1) SPECIFING THE LANGUAGE**"
      ],
      "metadata": {
        "id": "hSkXUWt7nSb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio4_Vocals.wav --language Chinese --task translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj_JrtfdnbDg",
        "outputId": "cee67cd6-31f9-43c7-9c47-66a41064f06f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "[00:00.000 --> 00:04.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:04.000 --> 00:06.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:06.000 --> 00:08.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:08.000 --> 00:10.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:10.000 --> 00:12.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:12.000 --> 00:14.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:14.000 --> 00:16.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:16.000 --> 00:18.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:18.000 --> 00:20.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:20.000 --> 00:22.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:22.000 --> 00:24.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:24.000 --> 00:26.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:26.000 --> 00:28.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:28.000 --> 00:30.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:30.000 --> 00:32.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:32.000 --> 00:34.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:34.000 --> 00:36.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:38.000 --> 00:40.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:40.000 --> 00:42.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:42.000 --> 00:44.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:44.000 --> 00:46.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:46.000 --> 00:48.000]  This is the first time I've ever seen someone like this in my life.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0915 H 02 AUG 2023 (01) (1)**"
      ],
      "metadata": {
        "id": "B3sOOKG3wbSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio5_Vocals.wav --task translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLxeIga3wdnJ",
        "outputId": "a68721e1-4aca-4528-98ee-5a5bf4cc4846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: Javanese\n",
            "[00:00.000 --> 00:30.000]  I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't know what you're talking about, I don't\n",
            "[00:30.000 --> 00:47.600]  know what you're talking about, I don't know what you're talking about, I haven't seen you.\n",
            "[00:47.600 --> 01:00.660]  the\n",
            "[01:00.660 --> 01:01.400]  now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0915 H 02 AUG 2023 (01) (1) SPECIFING THE LANGUAGE**"
      ],
      "metadata": {
        "id": "anc8T-VzxOb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio5_Vocals.wav --language Chinese --task translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nebONV5xUxv",
        "outputId": "45e83766-6d7c-414c-b1ac-a8adcaaaa921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:05.000]  This is the first time I've ever seen someone like this in my life.\n",
            "[00:05.000 --> 00:09.000]  I've never seen anyone like this in my life.\n",
            "[00:09.000 --> 00:13.000]  I've never seen anyone like this in my life.\n",
            "[00:13.000 --> 00:18.000]  I've never seen anyone like this in my life.\n",
            "[00:18.000 --> 00:22.000]  I've never seen anyone like this in my life.\n",
            "[00:22.000 --> 00:26.000]  I've never seen anyone like this in my life.\n",
            "[00:26.000 --> 00:29.000]  I've never seen anyone like this in my life.\n",
            "[00:29.000 --> 00:33.000]  I've never seen anyone like this in my life.\n",
            "[00:33.000 --> 00:37.000]  I've never seen anyone like this in my life.\n",
            "[00:37.000 --> 00:42.000]  I've never seen anyone like this in my life.\n",
            "[00:44.000 --> 00:48.000]  I've never seen anyone like this in my life.\n",
            "[00:48.000 --> 00:53.000]  I've never seen anyone like this in my life.\n",
            "[00:53.000 --> 00:58.200]  I don't believe those who think I'm am者 ...\n",
            "[00:58.200 --> 00:59.600]  Hope you enjoyed watching this video.\n",
            "[00:59.740 --> 01:01.600]  Happy QQ gathering!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0950 H   02 AUG 2023 (01) (1)**"
      ],
      "metadata": {
        "id": "S1NYlRfKzFA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio6_Vocals.wav --task translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HWD27VfzJh4",
        "outputId": "ef172fa7-d3f5-4d02-cf69-d9fff63910f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: English\n",
            "[00:00.000 --> 00:30.000]  Yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah\n",
            "[00:30.000 --> 00:35.680]  yeah\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0950 H 02 AUG 2023 (01) (1) SPECIFING THE LANGUAGE**"
      ],
      "metadata": {
        "id": "WMxMBfMizN-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio6_Vocals.wav --language Chinese --task translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da-yMdKCzTHQ",
        "outputId": "14d0b15a-7226-459f-a545-556c815406b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:06.000]  This is the first time I've ever seen a person like this in my life.\n",
            "[00:06.000 --> 00:10.000]  This is the first time I've ever seen a person like this in my life.\n",
            "[00:10.000 --> 00:14.000]  This is the first time I've ever seen a person like this in my life.\n",
            "[00:14.000 --> 00:18.000]  This is the first time I've ever seen a person like this in my life.\n",
            "[00:18.000 --> 00:22.000]  This is the first time I've ever seen a person like this in my life.\n",
            "[00:22.000 --> 00:26.000]  This is the first time I've ever seen a person like this.\n",
            "[00:26.000 --> 00:31.000]  This is the first time I've ever seen a person like this in my life.\n",
            "[00:31.000 --> 00:36.000]  This is the first time I've ever seen a person like this in my life.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1350 H  02 AUG 2023 (01)**"
      ],
      "metadata": {
        "id": "zgMzh2_Tzr2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio7_Vocals.wav --task translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaVaH-3rzvUI",
        "outputId": "9fbd0617-2aff-4444-ef81-fced1c1d4d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: Myanmar\n",
            "[00:00.000 --> 00:30.000]  It is very difficult to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a\n",
            "[00:30.000 --> 00:44.080]  way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find the way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a way to find a dog.\n",
            "[01:30.000 --> 01:40.000]  I'm going to show you how to do it.\n",
            "[01:40.000 --> 01:50.000]  I'm going to show you how to do it.\n",
            "[01:50.000 --> 01:58.000]  I'm going to show you how to do it.\n",
            "[01:58.000 --> 02:04.000]  I'm going to show you how to do it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1350 H 02 AUG 2023 (01) SPECIFING THE LANGUAGE**"
      ],
      "metadata": {
        "id": "zObSkSsiz7UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio7_Vocals.wav --language Chinese --task translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzjBEA7d0ANY",
        "outputId": "545b3474-3e26-433a-e15e-c92798eab4df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:30.000]  It's a good thing that I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I\n",
            "[00:30.000 --> 00:50.740]  Here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here, I'm here,\n",
            "[00:50.740 --> 01:20.740]  No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no\n",
            "[01:20.740 --> 01:29.700]  no, no, no, no, no\n",
            "[01:29.700 --> 01:59.700]  I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't\n",
            "[01:59.700 --> 02:05.410]  I don't know what I'm talking about, I don't know\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1405 H   AS ON 02 AUG 2023 (01) (1)**"
      ],
      "metadata": {
        "id": "eU4dplVJ0hFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio8_Vocals.wav --task translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_noq6xjw0j4_",
        "outputId": "4c3ac8ce-c107-44b8-c4dc-c66e3dbab417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: Hawaiian\n",
            "[00:00.000 --> 00:30.000]  I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm\n",
            "[00:30.000 --> 00:39.040]  going to say, but I don't know what I'm going to say, but I don't know what I'm going to\n",
            "[00:40.000 --> 00:56.920]  say, but I don't know what I'm going to say, but I don't know what I'm going to say, but I don't know what I'm\n",
            "[00:56.920 --> 01:26.920]  I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't know what I'm talking about, I don't\n",
            "[01:26.920 --> 01:31.120]  know what I'm talking about, I don't know what I'm talking about.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1405 H AS ON 02 AUG 2023 (01) (1) SPECIFING THE LANGUAGE**"
      ],
      "metadata": {
        "id": "cSO0NXZF00pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio8_Vocals.wav --language Chinese --task translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHk2yA_e0336",
        "outputId": "f701d01f-772f-4d3d-cabe-62b0cc924896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:02.000]  You are not going to be able to do it.\n",
            "[00:02.000 --> 00:04.000]  You are not going to be able to do it.\n",
            "[00:04.000 --> 00:06.000]  You are not going to be able to do it.\n",
            "[00:06.000 --> 00:08.000]  You are not going to be able to do it.\n",
            "[00:08.000 --> 00:10.000]  You are not going to be able to do it.\n",
            "[00:10.000 --> 00:12.000]  You are not going to be able to do it.\n",
            "[00:12.000 --> 00:14.000]  You are not going to be able to do it.\n",
            "[00:14.000 --> 00:16.000]  You are not going to be able to do it.\n",
            "[00:16.000 --> 00:18.000]  You are not going to be able to do it.\n",
            "[00:18.000 --> 00:20.000]  You are not going to be able to do it.\n",
            "[00:20.000 --> 00:22.000]  You are not going to be able to do it.\n",
            "[00:22.000 --> 00:24.000]  You are not going to be able to do it.\n",
            "[00:24.000 --> 00:26.000]  You are not going to be able to do it.\n",
            "[00:26.000 --> 00:28.000]  You are not going to be able to do it.\n",
            "[00:28.000 --> 00:30.000]  You are not going to be able to do it.\n",
            "[00:30.000 --> 00:32.000]  You are going to be able to do it.\n",
            "[00:32.000 --> 00:34.000]  You are not going to be able to do it.\n",
            "[00:34.000 --> 00:36.000]  You will not be able to do it.\n",
            "[00:36.000 --> 00:38.000]  You will not be able to do it.\n",
            "[00:38.000 --> 00:40.000]  You will not be able to do it.\n",
            "[00:40.000 --> 00:42.000]  You can't do it.\n",
            "[00:42.000 --> 00:44.000]  You are not going to be able to do it.\n",
            "[00:44.000 --> 00:46.000]  You will be able to do it.\n",
            "[00:46.000 --> 00:48.000]  You can't do it.\n",
            "[00:48.000 --> 00:50.000]  You are not going to be able to do it.\n",
            "[00:50.000 --> 00:52.000]  You are not going to be able to do it.\n",
            "[00:52.000 --> 00:54.000]  You are not going to be able to do it.\n",
            "[00:54.000 --> 00:58.000]  This is the first time I've seen a person like this in my life.\n",
            "[00:58.000 --> 01:02.000]  This is the first time I've seen a person like this in my life.\n",
            "[01:02.000 --> 01:06.000]  This is the first time I've seen a person like this in my life.\n",
            "[01:06.000 --> 01:10.000]  This is the first time I've seen a person like this in my life.\n",
            "[01:10.000 --> 01:14.000]  This is the first time I've seen a person like this in my life.\n",
            "[01:14.000 --> 01:18.000]  This is the first time I've seen a person like this in my life.\n",
            "[01:18.000 --> 01:22.000]  This is the first time I've seen a person like this in my life.\n",
            "[01:22.000 --> 01:27.000]  This is the first time I've seen a person like this in my life.\n",
            "[01:27.000 --> 01:31.000]  This is the first time I've seen a person like this in my life.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1635 H   AS ON 02 AUG 2023 (01) (1)**"
      ],
      "metadata": {
        "id": "1ZJcxH4p1Dav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio9_Vocals.wav --task translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSGGJzuN1Gl4",
        "outputId": "d488f2bf-ec10-4fd9-a350-773aa5806dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: Sindhi\n",
            "[00:00.000 --> 00:04.000]  I don't know why I'm all over it.\n",
            "[00:04.000 --> 00:08.000]  If you think that I'm a person who can't do anything,\n",
            "[00:08.000 --> 00:12.000]  can't do anything, I can't do anything, I can't do anything,\n",
            "[00:12.000 --> 00:16.000]  I can't do anything, I can't do anything,\n",
            "[00:16.000 --> 00:20.000]  I can't do anything, I can't do anything, I can't do anything,\n",
            "[00:20.000 --> 00:24.000]  I can't do anything, I can't do anything, I can't do anything,\n",
            "[00:24.000 --> 00:28.000]  I can't do anything, I can't do anything,\n",
            "[00:28.000 --> 00:34.000]  I can't say anything,\n",
            "[00:34.000 --> 00:48.000]  but you must always come and do what is best in your life.\n",
            "[00:48.000 --> 00:52.000]  Before you go, you must always do what is best in your life.\n",
            "[00:52.000 --> 00:56.000]  That's the principle we have known.\n",
            "[00:56.000 --> 01:03.420]  What you are trying to do is\n",
            "[01:03.420 --> 01:11.920]  Wow.\n",
            "[01:11.920 --> 01:16.000]  I'm\n",
            "[01:16.000 --> 01:26.720]  Amen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1635 H AS ON 02 AUG 2023 (01) (1) SPECIFING THE LANGUAGE**"
      ],
      "metadata": {
        "id": "OffNtlOv1HFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio9_Vocals.wav --language Chinese --task translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n5wj--P1Ml7",
        "outputId": "243da010-4cb0-4d61-cc38-b8f61b61522e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:04.000]  This is a very important thing.\n",
            "[00:04.000 --> 00:12.000]  If you want to go to China, you have to go to China.\n",
            "[00:12.000 --> 00:18.000]  If you want to go to China, you have to go to China.\n",
            "[00:18.000 --> 00:26.000]  If you want to go to China, you have to go to China.\n",
            "[00:26.000 --> 00:34.000]  If you want to go to China, you have to go to China.\n",
            "[00:34.000 --> 00:43.000]  If you want to go to China, you have to go to China.\n",
            "[00:43.000 --> 00:51.940]  I may have fallen illegitimate child, but it is difficult to get a child.\n",
            "[00:51.940 --> 00:55.360]  This is a due delay of the marriage couple.\n",
            "[00:55.500 --> 01:01.600]  So they've spent a place on their family's background.\n",
            "[01:01.600 --> 01:05.320]  We eat everything.\n",
            "[01:05.380 --> 01:10.520]  Just tell the sea-wrestling god what you want,\n",
            "[01:10.580 --> 01:15.260]  and tell people about his reality.\n",
            "[01:15.300 --> 01:18.960]  If God allows do that,\n",
            "[01:18.960 --> 01:23.600]  God can do whatever He wants.\n",
            "[01:23.620 --> 01:26.300]  Please call me.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1640 H  AS ON 02 AUG 2023 (02) (1)**"
      ],
      "metadata": {
        "id": "ef0x7b4R1S_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio10_Vocals.wav --task translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUVo30WA1WNZ",
        "outputId": "68d4f65b-cacb-46c1-b4d4-f21e0d425a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: Myanmar\n",
            "[00:00.000 --> 00:03.160]  That's not going to happen.\n",
            "[00:03.240 --> 00:06.560]  It isn't going to happen.\n",
            "[00:06.640 --> 00:09.400]  It's not going to happen.\n",
            "[00:09.460 --> 00:12.840]  It's not going to happen.\n",
            "[00:12.880 --> 00:23.000]  Nothing of it is going to happen.\n",
            "[00:23.000 --> 00:27.640]  VMware is the most important organizations in the world,\n",
            "[00:27.640 --> 00:38.580]  so many active startup organizations have invited people to support innovation.\n",
            "[00:38.580 --> 00:42.240]  Where does innovation come from?\n",
            "[00:42.240 --> 00:48.220]  Sort out the idea of American people in India\n",
            "[00:48.220 --> 00:52.420]  building doncs and refurbiiing,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1640 H AS ON 02 AUG 2023 (02) (1) SPECIFING THE LANGUAGE**"
      ],
      "metadata": {
        "id": "I_ECzqWZ1ayA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper output/audio10_Vocals.wav --language Chinese --task translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8pWYQKo1aMK",
        "outputId": "f2dacc28-c7e6-4f78-c2dd-550c07e34ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:06.660]  If I am Face, I have to give up\n",
            "[00:06.660 --> 00:12.980]  If I am Face, I can't find myself\n",
            "[00:12.980 --> 00:16.900]  or not, I criticize\n",
            "[00:16.900 --> 00:20.640]  But I can't help\n",
            "[00:20.640 --> 00:23.280]  I am fine\n",
            "[00:23.280 --> 00:26.680]  is it gone?\n",
            "[00:26.680 --> 00:29.120]  I can say\n",
            "[00:29.120 --> 00:31.360]  Madam who has taken away the English rustic painting.\n",
            "[00:32.420 --> 00:37.300]  Is it a joke to not use it to draw things on the table?\n",
            "[00:38.740 --> 00:41.160]  Of course it is.\n",
            "[00:42.180 --> 00:44.840]  I am just here to do farming.\n",
            "[00:45.860 --> 00:47.260]  Come at all.\n",
            "[00:47.780 --> 00:48.840]  Here.\n",
            "[00:49.340 --> 00:50.240]  Come.\n"
          ]
        }
      ]
    }
  ]
}